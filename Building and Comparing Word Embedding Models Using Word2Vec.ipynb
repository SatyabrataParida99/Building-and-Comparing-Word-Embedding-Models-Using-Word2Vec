{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d708bd93-b136-4a72-a364-d85edb7c1be2",
   "metadata": {},
   "source": [
    "<img src=\"https://online.york.ac.uk/wp-content/uploads/2021/10/Image-of-a-human-made-up-of-lit-up-lines-touching-a-graphic-which-reads-NLP.jpg\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 1280px; height: 197px; margin: 0.5px 0px; width: 351px;\" alt=\"The role of natural language processing in AI - University of York\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23eee7-0735-4528-ad99-f216403d8553",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240524132821/nlp-working.webp\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 1043px; height: 236px; margin: 0px; width: 330px;\" alt=\"Natural Language Processing (NLP) - Overview - GeeksforGeeks\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bab8f-ef87-46c6-a728-46fb082a7b5e",
   "metadata": {},
   "source": [
    "<img src=\"https://www.samyzaf.com/ML/nlp/word2vec2.png\" jsaction=\"\" class=\"sFlh5c FyHeAf iPVvYb\" style=\"max-width: 1951px; height: 185px; margin: 0px; width: 351px;\" alt=\"word2vec\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb017f-4ad5-40d8-8571-2247bc8356d3",
   "metadata": {},
   "source": [
    "**Word2Vec** is a technique in natural language processing (NLP) used to represent words as dense vectors of fixed size, where words with similar meanings are close in the vector space. The method was introduced by Tomas Mikolov and his team at Google in 2013. It is based on shallow neural networks that learn word representations by processing large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f1c74-f72b-4d99-bf04-3828ee7cc9b8",
   "metadata": {},
   "source": [
    "- There are two main architectures in Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3517cb-badf-4f3b-8ec1-1a41b32c1030",
   "metadata": {},
   "source": [
    "- **Continuous Bag of Words (CBOW):** Predicts the target word (center word) based on its context (surrounding words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e1472-2439-49c7-9878-2ef4d64295cf",
   "metadata": {},
   "source": [
    "- **Skip-gram:** The inverse of CBOW, where the model predicts the surrounding words based on the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e43f88-1367-4d16-9ace-d9c1d56885d8",
   "metadata": {},
   "source": [
    "**Key Points of Word2Vec:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79f0f1-0fbf-4cc5-8362-1667f5185cc8",
   "metadata": {},
   "source": [
    "- **Training Objective:** The model is trained to maximize the likelihood of the context words given a target word (Skip-gram) or vice versa (CBOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15bcf7-e07b-47a7-906e-605b664c891c",
   "metadata": {},
   "source": [
    "- **Efficient Learning:** Uses techniques like hierarchical softmax or negative sampling to efficiently train on large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a516da9-6e03-4de2-a2d5-b6d72f833cc2",
   "metadata": {},
   "source": [
    "- **Applications:** It is used in many NLP tasks like word similarity, analogy completion, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83476255-aa13-42fd-8604-b9697b918ad1",
   "metadata": {},
   "source": [
    "**# Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e58daf-b325-4918-a989-8bad41077008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # For NLP tasks like tokenization\n",
    "from gensim.models import Word2Vec  # To create and train Word2Vec models\n",
    "from nltk.corpus import stopwords  # To remove common stopwords from text\n",
    "import re  # For regular expressions to preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8db35-1fbd-4dc8-a82f-daf19c483e8a",
   "metadata": {},
   "source": [
    "**Sample paragraph for analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e6e8a3-5611-4a4b-8469-b8c3a016d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I hold three profound visions for India. Over the past 3000 years, our land has witnessed \n",
    "invasions from countless civilizations—Alexander, the Greeks, the Turks, the Mughals, the Portuguese, \n",
    "the British, the French, and the Dutch. They came, took over our lands, plundered our resources, \n",
    "and sought to dominate our minds. Yet, India has never invaded or oppressed any other nation. \n",
    "We have not taken away others’ land, culture, or history, nor forced our way of life upon them. \n",
    "Why? Because we cherish and respect freedom. This is my first vision for India: freedom. \n",
    "The seeds of this vision were sown in 1857 during the War of Independence. \n",
    "This freedom is precious, and it is our duty to safeguard and strengthen it. \n",
    "Without freedom, we cannot command dignity or respect.\n",
    "My second vision is for progress. For over five decades, we have been a developing nation. \n",
    "It is time to reimagine ourselves as a developed nation. India stands among the world’s top \n",
    "economies in GDP, with notable achievements in many sectors. Our poverty levels are reducing, \n",
    "and the world recognizes our advancements. Yet, we often hesitate to see ourselves as capable, \n",
    "self-reliant, and confident. This mindset must change if we are to embrace our true potential.\n",
    "My third vision is for India to stand tall on the global stage. Respect is earned through \n",
    "strength—both economic and military—and these must evolve together. Only when we are strong \n",
    "can we inspire respect and influence globally.\n",
    "I have been fortunate to work with extraordinary mentors like Dr. Vikram Sarabhai, Prof. \n",
    "Satish Dhawan, and Dr. Brahm Prakash. Their guidance shaped my journey and opened doors to \n",
    "remarkable opportunities. As I reflect on my career, I identify four significant milestones \n",
    "that have defined my path and contributed to my growth.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42f50d-8757-4fc0-9d11-e76af2a9c889",
   "metadata": {},
   "source": [
    "**Step 1: Text preprocessing**\n",
    "- Key Point: Preprocessing ensures data is clean and standardized for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943d1f4a-51b6-44a1-ba04-c46bd4dbb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\[[0-9]*\\]', ' ', paragraph)  # Remove references like [1], [2]\n",
    "text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "text = text.lower()  # Convert text to lowercase for uniformity\n",
    "text = re.sub(r'\\d', ' ', text)  # Remove digits from text\n",
    "text = re.sub(r'\\s+', ' ', text)  # Normalize spaces again after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8696e-1b42-4ab3-9c1a-057c39b89f98",
   "metadata": {},
   "source": [
    "**Step 2: Sentence tokenization**\n",
    "- Key Point: Tokenize the paragraph into sentences, then into words for Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1911c783-8f67-4b3d-a4cd-4c2e843769d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)  # Split text into sentences\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]  # Tokenize each sentence into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48089542-2316-4318-b8be-e0ca89d9cd65",
   "metadata": {},
   "source": [
    "**Step 3: Remove stopwords**\n",
    "- Key Point: Stopwords are common words (like 'the', 'is') that don't add value to context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a24de6e-3ea5-4a26-befb-c0a705ff5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb48884-3172-40fd-8f98-6f70c3167db8",
   "metadata": {},
   "source": [
    "**Step 4: Train the Word2Vec model**\n",
    "- Key Point: Word2Vec creates vector representations of words based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32273d5e-c603-4488-a180-6e103f126640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1)  # min_count=1 ensures all words are considered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ffdf1-4ddb-460a-9b82-cbadba9cbac7",
   "metadata": {},
   "source": [
    "**Extract vocabulary**\n",
    "- Key Point: Access vocabulary and its vector dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "422ea495-bf65-491c-8896-0c59eda4fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = model.wv.vocab  # List of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5c019-f20c-44bf-a1a1-83a82b77daf8",
   "metadata": {},
   "source": [
    "**Step 5: Finding word vectors**\n",
    "- Key Point: Vector representations capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d934028-d6ea-4c64-b34a-427c7ff4d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['war']  # Get the vector for the word 'war"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93037816-453c-4dbf-becf-008a29a1d935",
   "metadata": {},
   "source": [
    "**Step 6: Finding similar words**\n",
    "- Key Point: Find contextually similar words based on vector distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f854c9d-6c42-406f-ab59-67c026c4fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_to_war = model.wv.most_similar('war')\n",
    "similar_to_freedom = model.wv.most_similar('freedom')\n",
    "similar_to_vikram = model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6386c4c-e8a7-4b2d-9866-a8ce0536a9ff",
   "metadata": {},
   "source": [
    "**Step 7: Compare CBOW and Skip-gram models**\n",
    "- Key Point: CBOW predicts a target word from context; Skip-gram predicts context from a target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c20997a5-381e-4cfb-bceb-a5d6790b1ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language': [-9.58782248e-03  8.95247795e-03  4.16768529e-03  9.23937839e-03\n",
      "  6.63647708e-03  2.91263685e-03  9.80561227e-03 -4.41262405e-03\n",
      " -6.79557770e-03  4.21743933e-03  3.71244014e-03 -5.65997604e-03\n",
      "  9.70590487e-03 -3.55342729e-03  9.56569146e-03  8.24862858e-04\n",
      " -6.35078829e-03 -1.98813085e-03 -7.37798773e-03 -2.97638960e-03\n",
      "  1.04563322e-03  9.49254353e-03  9.36291739e-03 -6.61006477e-03\n",
      "  3.47603019e-03  2.29112501e-03 -2.50939350e-03 -9.23522189e-03\n",
      "  1.02926721e-03 -8.15827399e-03  6.33065309e-03 -5.79973916e-03\n",
      "  5.53055434e-03  9.82293021e-03 -1.69930747e-04  4.53570904e-03\n",
      " -1.81779568e-03  7.36540137e-03  3.93093657e-03 -9.01917554e-03\n",
      " -2.38957629e-03  3.64277582e-03 -1.00166544e-04 -1.21789996e-03\n",
      " -1.06380926e-03 -1.68953754e-03  6.03163266e-04  4.15880233e-03\n",
      " -4.25073365e-03 -3.83816310e-03 -4.38034949e-05  2.52343132e-04\n",
      " -1.72673841e-04 -4.79154196e-03  4.32580896e-03 -2.16904492e-03\n",
      "  2.10142788e-03  6.58018980e-04  5.97620010e-03 -6.85623055e-03\n",
      " -6.82329386e-03 -4.49230243e-03  9.45300516e-03 -1.59457617e-03\n",
      " -9.42957774e-03 -5.26159827e-04 -4.44914307e-03  6.00073999e-03\n",
      " -9.61587857e-03  2.87202443e-03 -9.26861539e-03  1.26039959e-03\n",
      "  6.01406815e-03  7.39096059e-03 -7.62542244e-03 -6.05646241e-03\n",
      " -6.84698252e-03 -7.91507959e-03 -9.49811563e-03 -2.12258520e-03\n",
      " -8.49981210e-04 -7.24436250e-03  6.78500999e-03  1.12896471e-03\n",
      "  5.83423767e-03  1.45897327e-03  8.07931705e-04 -7.35459011e-03\n",
      " -2.19200552e-03  4.32581734e-03 -5.07098483e-03  1.12398434e-03\n",
      "  2.88278796e-03 -1.55447971e-03  9.93423909e-03  8.34725704e-03\n",
      "  2.40527932e-03  7.13505130e-03  5.89500880e-03 -5.57549484e-03]\n",
      "Words similar to 'language': [('world', 0.2854067385196686), ('agents', 0.2405676394701004), ('AI', 0.19989998638629913), ('Supervised', 0.1901194304227829), ('Data', 0.1729680746793747)]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences for training in AI\n",
    "sentences = [\n",
    "    [\"Artificial\", \"intelligence\", \"is\", \"transforming\", \"the\", \"world\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"AI\"],\n",
    "    [\"Deep\", \"learning\", \"uses\", \"neural\", \"networks\", \"for\", \"pattern\", \"recognition\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"enables\", \"computers\", \"to\", \"understand\", \"human\", \"language\"],\n",
    "    [\"AI\", \"applications\", \"include\", \"image\", \"recognition\", \"and\", \"autonomous\", \"vehicles\"],\n",
    "    [\"Reinforcement\", \"learning\", \"helps\", \"train\", \"agents\", \"through\", \"reward\", \"systems\"],\n",
    "    [\"Data\", \"is\", \"the\", \"key\", \"fuel\", \"for\", \"artificial\", \"intelligence\"],\n",
    "    [\"Supervised\", \"learning\", \"requires\", \"labeled\", \"data\"],\n",
    "    [\"Unsupervised\", \"learning\", \"groups\", \"data\", \"based\", \"on\", \"similarities\"],\n",
    "    [\"Ethics\", \"in\", \"AI\", \"is\", \"important\", \"to\", \"ensure\", \"responsible\", \"development\"],\n",
    "]\n",
    "\n",
    "# KeyPoint: The training data consists of tokenized sentences. Each sentence is a list of words, providing the context for training Word2Vec.\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# KeyPoint:\n",
    "# - `vector_size=100`: The dimensionality of the word vectors (100 dimensions).\n",
    "# - `window=5`: The maximum distance between the target word and context words.\n",
    "# - `min_count=1`: Words that appear less than once are ignored.\n",
    "# - `sg=1`: Skip-gram model is used (if `sg=0`, CBOW would be used).\n",
    "\n",
    "# Get the vector for a word\n",
    "vector = model.wv['language']  # Fetch the vector representation for the word 'language'\n",
    "print(\"Vector for 'language':\", vector)\n",
    "\n",
    "# KeyPoint: Each word is represented as a dense vector in a high-dimensional space. The vector captures the word's semantic meaning.\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('language', topn=5)  # Find the top 5 words similar to 'language'\n",
    "print(\"Words similar to 'language':\", similar_words)\n",
    "\n",
    "# KeyPoint: `most_similar` computes similarity between word vectors. Words with similar vectors (semantic proximity) are identified.\n",
    "# - `topn=5`: Limits the output to the top 5 most similar words.\n",
    "\n",
    "# Overall KeyPoint:\n",
    "# - Word2Vec learns relationships between words based on their context in the sentences.\n",
    "# - Skip-gram (sg=1) focuses on predicting context words given a target word, which works well for smaller datasets.\n",
    "# - This trained model can now be used for tasks like finding word similarity or clustering words based on their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e93a4297-b1fd-4740-98ae-6f2a1ec4daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Vector for 'language': [-9.5778247e-03  8.9507308e-03  4.1638282e-03  9.2415949e-03\n",
      "  6.6409362e-03  2.9237177e-03  9.8145939e-03 -4.4304705e-03\n",
      " -6.8065114e-03  4.2280448e-03  3.7324817e-03 -5.6659030e-03\n",
      "  9.7130537e-03 -3.5615030e-03  9.5540686e-03  8.2726392e-04\n",
      " -6.3318592e-03 -1.9830721e-03 -7.3820893e-03 -2.9793170e-03\n",
      "  1.0386818e-03  9.4887791e-03  9.3572065e-03 -6.6004116e-03\n",
      "  3.4746295e-03  2.2733128e-03 -2.4985084e-03 -9.2301974e-03\n",
      "  1.0292478e-03 -8.1671849e-03  6.3251727e-03 -5.8021853e-03\n",
      "  5.5378280e-03  9.8346984e-03 -1.6494206e-04  4.5241606e-03\n",
      " -1.8167844e-03  7.3680729e-03  3.9445432e-03 -9.0172039e-03\n",
      " -2.4024302e-03  3.6295475e-03 -9.7236669e-05 -1.2063734e-03\n",
      " -1.0603560e-03 -1.6788138e-03  6.0107547e-04  4.1653048e-03\n",
      " -4.2543472e-03 -3.8350383e-03 -5.2279724e-05  2.6976145e-04\n",
      " -1.7055863e-04 -4.7903457e-03  4.3160366e-03 -2.1736391e-03\n",
      "  2.1093355e-03  6.6396361e-04  5.9701521e-03 -6.8402458e-03\n",
      " -6.8201255e-03 -4.4861357e-03  9.4411829e-03 -1.5910413e-03\n",
      " -9.4370022e-03 -5.4367271e-04 -4.4519799e-03  6.0025118e-03\n",
      " -9.5969774e-03  2.8592758e-03 -9.2577934e-03  1.2516673e-03\n",
      "  6.0047898e-03  7.3928973e-03 -7.6222075e-03 -6.0544061e-03\n",
      " -6.8394840e-03 -7.9155127e-03 -9.5062293e-03 -2.1271587e-03\n",
      " -8.3602715e-04 -7.2568115e-03  6.7936159e-03  1.1287690e-03\n",
      "  5.8349897e-03  1.4689928e-03  7.9077668e-04 -7.3672449e-03\n",
      " -2.1711024e-03  4.3261321e-03 -5.0851153e-03  1.1293215e-03\n",
      "  2.8851104e-03 -1.5438885e-03  9.9399658e-03  8.3500305e-03\n",
      "  2.4203486e-03  7.1249739e-03  5.8971364e-03 -5.5852132e-03]\n",
      "Skip-gram Vector for 'language': [-9.5896255e-03  8.9502595e-03  4.1477759e-03  9.2406925e-03\n",
      "  6.6353604e-03  2.9228693e-03  9.8115494e-03 -4.4289902e-03\n",
      " -6.8187490e-03  4.2381999e-03  3.7334962e-03 -5.6650965e-03\n",
      "  9.7143780e-03 -3.5541125e-03  9.5580490e-03  8.1610295e-04\n",
      " -6.3212616e-03 -2.0030804e-03 -7.3835319e-03 -2.9767156e-03\n",
      "  1.0543938e-03  9.4860466e-03  9.3709584e-03 -6.6040321e-03\n",
      "  3.4723319e-03  2.2706722e-03 -2.5051895e-03 -9.2284596e-03\n",
      "  1.0322878e-03 -8.1642037e-03  6.3353404e-03 -5.8003319e-03\n",
      "  5.5438061e-03  9.8251933e-03 -1.5909059e-04  4.5252624e-03\n",
      " -1.8227316e-03  7.3830290e-03  3.9465050e-03 -9.0129366e-03\n",
      " -2.4008632e-03  3.6346524e-03 -9.3851246e-05 -1.2006829e-03\n",
      " -1.0626625e-03 -1.6809840e-03  6.0690247e-04  4.1718590e-03\n",
      " -4.2482316e-03 -3.8367815e-03 -4.7042562e-05  2.7559832e-04\n",
      " -1.6195563e-04 -4.7961432e-03  4.3290937e-03 -2.1798855e-03\n",
      "  2.1128790e-03  6.7515793e-04  5.9804791e-03 -6.8472503e-03\n",
      " -6.8206852e-03 -4.4985185e-03  9.4454959e-03 -1.5896056e-03\n",
      " -9.4429106e-03 -5.4604484e-04 -4.4466327e-03  6.0075638e-03\n",
      " -9.6040256e-03  2.8721450e-03 -9.2599932e-03  1.2477424e-03\n",
      "  6.0075298e-03  7.3949937e-03 -7.6243007e-03 -6.0589672e-03\n",
      " -6.8401741e-03 -7.9130335e-03 -9.5013846e-03 -2.1259869e-03\n",
      " -8.5043511e-04 -7.2563444e-03  6.7894631e-03  1.1251622e-03\n",
      "  5.8376063e-03  1.4575511e-03  7.9126377e-04 -7.3618242e-03\n",
      " -2.1697553e-03  4.3373452e-03 -5.0780294e-03  1.1256417e-03\n",
      "  2.8881817e-03 -1.5511364e-03  9.9449567e-03  8.3471062e-03\n",
      "  2.4229323e-03  7.1404777e-03  5.8973734e-03 -5.5895187e-03]\n",
      "CBOW - Words similar to 'language': [('world', 0.2856367826461792), ('agents', 0.24009577929973602), ('AI', 0.19993196427822113), ('Supervised', 0.19027791917324066), ('Data', 0.17222899198532104)]\n",
      "Skip-gram - Words similar to 'language': [('world', 0.28575465083122253), ('agents', 0.24059388041496277), ('AI', 0.19936563074588776), ('Supervised', 0.18989086151123047), ('Data', 0.17222756147384644)]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    [\"Artificial\", \"intelligence\", \"is\", \"transforming\", \"the\", \"world\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"AI\"],\n",
    "    [\"Deep\", \"learning\", \"uses\", \"neural\", \"networks\", \"for\", \"pattern\", \"recognition\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"enables\", \"computers\", \"to\", \"understand\", \"human\", \"language\"],\n",
    "    [\"AI\", \"applications\", \"include\", \"image\", \"recognition\", \"and\", \"autonomous\", \"vehicles\"],\n",
    "    [\"Reinforcement\", \"learning\", \"helps\", \"train\", \"agents\", \"through\", \"reward\", \"systems\"],\n",
    "    [\"Data\", \"is\", \"the\", \"key\", \"fuel\", \"for\", \"artificial\", \"intelligence\"],\n",
    "    [\"Supervised\", \"learning\", \"requires\", \"labeled\", \"data\"],\n",
    "    [\"Unsupervised\", \"learning\", \"groups\", \"data\", \"based\", \"on\", \"similarities\"],\n",
    "    [\"Ethics\", \"in\", \"AI\", \"is\", \"important\", \"to\", \"ensure\", \"responsible\", \"development\"],\n",
    "]\n",
    "\n",
    "# KeyPoint: These sentences form the corpus for training Word2Vec models. Each list represents a tokenized sentence.\n",
    "\n",
    "# CBOW Model\n",
    "cbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)\n",
    "# KeyPoint:\n",
    "# - CBOW (Continuous Bag of Words) predicts a target word using its surrounding context.\n",
    "# - `vector_size=100`: Dimensionality of the word vectors.\n",
    "# - `window=2`: The context window size (words within 2 positions before and after the target word are considered).\n",
    "# - `min_count=1`: Ignores words that appear less than once.\n",
    "# - `sg=0`: Specifies CBOW model (sg=1 would specify Skip-gram).\n",
    "\n",
    "# Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n",
    "# KeyPoint:\n",
    "# - Skip-gram predicts surrounding context words for a given target word.\n",
    "# - The parameters are the same as CBOW, except `sg=1` for Skip-gram.\n",
    "\n",
    "# Example: Getting the vector for a word\n",
    "word = \"language\"\n",
    "cbow_vector = cbow_model.wv[word]  # Fetch vector for 'language' using CBOW model\n",
    "skipgram_vector = skipgram_model.wv[word]  # Fetch vector for 'language' using Skip-gram model\n",
    "\n",
    "print(f\"CBOW Vector for '{word}':\", cbow_vector)\n",
    "print(f\"Skip-gram Vector for '{word}':\", skipgram_vector)\n",
    "\n",
    "# KeyPoint: \n",
    "# - Each word is represented as a dense vector of `vector_size` dimensions.\n",
    "# - The vectors are learned based on the context in the training sentences.\n",
    "\n",
    "# Example: Finding similar words\n",
    "cbow_similar_words = cbow_model.wv.most_similar(word, topn=5)  # Find top 5 similar words using CBOW model\n",
    "skipgram_similar_words = skipgram_model.wv.most_similar(word, topn=5)  # Find top 5 similar words using Skip-gram model\n",
    "\n",
    "print(f\"CBOW - Words similar to '{word}':\", cbow_similar_words)\n",
    "print(f\"Skip-gram - Words similar to '{word}':\", skipgram_similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe79c5c-24fe-4d06-8a98-e6760738f94e",
   "metadata": {},
   "source": [
    "# KeyPoint:\n",
    " - `most_similar` identifies words with similar vector representations, indicating semantic similarity.\n",
    " - The similarity results might differ between CBOW and Skip-gram due to differences in training objectives.\n",
    " - CBOW focuses on context, making it faster for small datasets.\n",
    " - Skip-gram works well for smaller data and captures rare word relationships effectively.\n",
    "\n",
    "# Overall KeyPoint:\n",
    " - CBOW is faster and performs better on frequent words.\n",
    " - Skip-gram is slower but better for learning rare word embeddings.\n",
    " - Both models learn the contextual and semantic relationships between words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16642fb4-c185-44ef-9913-fd0728918621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
